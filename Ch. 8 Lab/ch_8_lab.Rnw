\documentclass[12pt]{article}
 
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amsthm,amssymb, mathtools}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{fixltx2e}
\usepackage[shortlabels]{enumitem}
 
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}

\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{lemma}[2][Lemma]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{question}[2][Question]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{corollary}[2][Corollary]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newcommand{\textfrac}[2]{\dfrac{\text{#1}}{\text{#2}}}

\begin{document}

\title{Ch. 8 - Tree-Based Methods}

\author{Chris Hayduk}
\date{\today}

\maketitle

<<echo=F, results = 'hide', warning=FALSE, message=FALSE>>=
# set the global chunk options
opts_chunk$set(message=FALSE, # don't print R messages in pdf -- CHANGE TO FALSE WHEN SUBMITTING FINAL VERSION!!!
               warning=FALSE) # don't print warnings in pdf -- CHANGE TO FALSE WHEN SUBMITTING FINAL VERSION!!!

# load .RData files, .R files, etc.
# R packages required
suppressMessages(library(tidyverse, quietly=TRUE, warn.conflicts = FALSE, verbose=FALSE))
suppressMessages(library(ISLR, quietly=TRUE, warn.conflicts = FALSE, verbose=FALSE))
suppressMessages(library(splines, quietly=TRUE, warn.conflicts = FALSE, verbose=FALSE))
suppressMessages(library(gam, quietly=TRUE, warn.conflicts = FALSE, verbose=FALSE))
suppressMessages(library(akima, quietly=TRUE, warn.conflicts = FALSE, verbose=FALSE))
suppressMessages(library(tree, quietly=TRUE, warn.conflicts = FALSE, verbose=FALSE))
suppressMessages(library(MASS, quietly=TRUE, warn.conflicts = FALSE, verbose=FALSE))
suppressMessages(library(randomForest, quietly=TRUE, warn.conflicts = FALSE, verbose=FALSE))
suppressMessages(library(gbm, quietly=TRUE, warn.conflicts = FALSE, verbose=FALSE))

@

\section{Fitting Classification Trees}

We first use classification trees to analyze the \texttt{Carseats} data set. In these data, \texttt{Sales} is a continuous variable, and so we begin by recoding it as a binary variable. We use the \texttt{ifelse()} function to create a variable, called \texttt{High}, which takes on a value of Yes if the \texttt{Sales} variable exceeds 8, and takes on a value of No otherwise,

<<>>=
data(Carseats)
attach(Carseats)
High = ifelse(Sales<=8, "No", "Yes")
@

Finally, we use the \texttt{data.frame()} function to merge \texttt{High} with the rest of the \texttt{Carseats} data.

<<>>=
Carseats = data.frame(Carseats, High=as.factor(High))
@

We now use the \texttt{tree()} function to fit a classification tree in order to predict \texttt{High} using all variables but \texttt{Sales}. The syntax of the \texttt{tree()} function is quite similar to that of the \texttt{lm()} function.

<<>>=
tree.carseats = tree(High~.-Sales, data=Carseats)
@

The \texttt{Summary()} function lists the variables that are used as internal nodes in the tree, the number of terminal nodes, and the training error rate.
<<>>=
summary(tree.carseats)
@

We see that the training error rate is 9\%. For classification trees, the deviance reported in the output of \texttt{summary()} is given by,
\begin{align}
-2 \sum_m \sum_k n_{mk} \log \hat{p}_{mk}
\end{align}

where $n_{mk}$ is the number of observations in the $m$th terminal node that belong to the $k$th class. A small deviance indicates a tree that provides a good fit to the (training) data. The \textit{residual mean deviance} reported is simply the deviance divided by $n - |T_0|$, which in this case is $400-27=373$.


One of the most attractive properties of trees is that they can be graphically displayed. We use the \texttt{plot()} function to display the tree structure, and the \texttt{text()} function to display the node labels. The argument \texttt{pretty=0} instructs \texttt{R} to include the category names for any qualitative predictors, rather than simply displaying a letter for each category.

<<>>=
plot(tree.carseats)
text(tree.carseats, pretty = 0)
@

The most important indicator of \texttt{Sales} appears to be shelving location, since the first branch differentiates Good locations from Bad and Medium locations.

If we just type the name of the tree object, \texttt{R} prints output corresponding to each branch of the tree. \texttt{R} displays the split criterion (e.g. \texttt{Price<92.5}), the number of observations in that branch, the deviance the overall preduction for that branch (Yes or No), and the fraction of observations in that branch that take on values of Yes and No. Branches that lead to terminal nodes are indicated using asteriks.

<<>>=
tree.carseats
@

In order to properly evaluate the performance of a classification tree on these data, we must estimate the test error rather than simply computing the training error. We split the observations into a training set and a test set, build the tree using the training set, and evaluate its performance on the test data.
<<>>=
set.seed(2)
train = sample(1:nrow(Carseats), 200)
Carseats.test = Carseats[-train,]
High.test = High[-train]
tree.carseats=tree(High~.-Sales, Carseats, subset = train)
tree.pred = predict(tree.carseats, Carseats.test, type="class")
table(tree.pred, High.test)
@

Next, we consider whether pruning the tree might lead to improved results. The function \texttt{cv.tree()} performs cross validation in order to determine the optimal level of tree complexity.

<<>>=
set.seed(3)
cv.carseats=cv.tree(tree.carseats,FUN=prune.misclass)
names(cv.carseats)
@

Note that, despite the name, \texttt{dev} corresponds to the cross-validation error rate in this instance. The tree with 9 terminal nodes results in the lowest cross-validation error rate, with 50 cross-validation errors. We plot the error rate as a function of both \texttt{size} and \texttt{k}.

<<>>=
par(mfrow=c(1,2))
plot(cv.carseats$size,cv.carseats$dev,type="b")
plot(cv.carseats$k, cv.carseats$dev, type="b")
@

We now apply the \texttt{prune.misclass()} function in order to prune the tree to obtain the nine-node tree.

<<>>=
prune.carseats=prune.misclass(tree.carseats,best=9)
plot(prune.carseats)
text(prune.carseats, pretty=0)
@

How well does this pruned tree perform on the test data set? Once again, we apply the \texttt{predict()} function.

<<>>=
tree.pred=predict(prune.carseats, Carseats.test, type="class")
table(tree.pred, High.test)
@

Now 77\% of the test observations are correctly classified, so not only has the pruning process produced a more interpretable tree, but it has also improved the classification accuracy.

If we increase the value of \texttt{best}, we obtain a larger pruned tree with lower classification accuracy:
<<>>=
prune.carseats = prune.misclass(tree.carseats, best = 15)
plot(prune.carseats)
text(prune.carseats, pretty=0)
tree.pred=predict(prune.carseats, Carseats.test, type="class")
table(tree.pred, High.test)
@


\newpage
\section{Fitting Regression Trees}

Here we fit a regression tree to the \texttt{Boston} data set. First, we create a training set, and fit the tree to the training set.

<<>>=
set.seed(1)
train = sample(1:nrow(Boston), nrow(Boston)/2)
tree.boston = tree(medv~., Boston, subset=train)
summary(tree.boston)
@

Note that the output of \texttt{summary()} indicates that only three of the variables have been used in constructing the tree. In the context of a regression tree, the deviance is simply the sum of squared errors for the tree. We now plot the tree.

<<>>=
plot(tree.boston)
text(tree.boston, pretty=0)
@

The variable \texttt{lstat} measures the percentage of individuals with lower socioeconomic status. The tree indicates that lower values of \texttt{lstat} correspond to more expensive houses. The tree predicts a median house price of \$46,400 for larger homes in suburbs in which residents have high socioeconomic status.

Now we use the \texttt{cv.tree()} function to see whether pruning the tree will improve performance.

<<>>=
cv.boston = cv.tree(tree.boston)
plot(cv.boston$size, cv.boston$dev, type='b')
@

In this case, the most complex tree is selected by cross-validation. We can also prune the tree as follows:

<<>>=
yhat = predict(tree.boston, newdata=Boston[-train,])
boston.test = Boston[-train, "medv"]
plot(yhat, boston.test)
abline(0,1)
mean((yhat-boston.test)^2)
@

In other words, the test set MSE associated with the regression tree is 25.05. The square root of the MSE is therefore around 5.005, indicating that this model leads to test predictions that are within around \$5,005 of the true median home value for the suburb.

\newpage
\section{Bagging and Random Forests}

Here we apply bagging and random forests to the \texttt{Boston} data, using the \texttt{randomForest} package in \texttt{R}. The exact results obtained in this section may depend on the version of \texttt{R} and the version of the \texttt{randomForest} package. Recall that bagging is simply a special case of random forest with $m = p$. Therefore, the \texttt{randomForest()} function can be used to perform both random forests and bagging. We perform bagging as follows:

<<>>=
set.seed(1)
bag.boston = randomForest(medv~., 
                          data=Boston, 
                          subset=train, 
                          mtry=13, 
                          importance=TRUE)
bag.boston
@

The argument \texttt{mtry=13} indicates that all $13$ predictors should be considered for each split of the tree - in other words, that bagging should be done. How well does this bagged model perform on the test set?

<<>>=
yhat.bag = predict(bag.boston, newdata=Boston[-train,])
plot(yhat.bag, boston.test)
abline(0,1)
mean((yhat.bag-boston.test)^2)
@

The test MSE associated with the bagged regression tree is 13.16, almost half that obtained using an optimally-pruned single tree. We could change the number of trees grown by \texttt{randomForest()} using the \texttt{ntree} argument:

<<>>=
bag.boston = randomForest(medv~., data=Boston, 
                          subset=train, mtry=13,ntree=25)
yhat.bag = predict(bag.boston, newdata=Boston[-train,])
mean((yhat.bag-boston.test)^20)
@

Growing a random forest proceeds in exactly the same way, except that we use a smaller value of the \texttt{mtry} argument. By default, \texttt{randomForest()} uses $p/3$ variables when building a random forest of regression trees and $\sqrt{p}$ variables when building a random forest of classification trees. Here we use \texttt{mtry=6}.

<<>>=
set.seed(1)
rf.boston=randomForest(medv~., data=Boston, 
                       subset=train, mtry=6, 
                       importance = TRUE)
yhat.rf = predict(rf.boston, newdata=Boston[-train,])
mean((yhat.rf-boston.test)^2)
@

The test set MSE is 11.31; this indicates that random forest yielded an improvement over bagging in this case. 

Using the \texttt{importance()} function, we can view the importance of each variable:

<<>>=
importance(rf.boston)
@

The measures of variable importance are reported. The former is based upon the mean decrease of accuracy in predictions on the out of bag samples when a given variable is excluded from the model. The latter is a measure of the total decrease in node impurity that results from splits over that variable, averaged over all trees. In the case of regression trees, the node impurity is measured by the training RSS, and for classification trees by the deviance. Plots of these importance measures can be produced using the \texttt{varImpPlot()} function:

<<>>=
varImpPlot(rf.boston)
@

\newpage
\section{Boosting}

Here we use the \texttt{gbm} package, and within it \texttt{gbm()} function, to fit boosted regression trees to the \texttt{Boston} data set. We run \texttt{gbm()} with the option \texttt{distribution=``gaussian''} since this is a regression problem; if it were a binary classification problem, we would use \texttt{distribution=``bernoulli''}. The argument \texttt{n.trees=5000} indicates that we want 5000 trees, and the option \texttt{interaction.depth=4} limits the depth of each tree.

<<>>=
set.seed(1)
boost.boston=gbm(medv~., data=Boston[train,], 
                 distribution="gaussian", n.trees=5000, 
                 interaction.depth=4)
@

The \texttt{summary()} function produces a relative influence plot and also outputs the relative influence statistics.

<<>>=
summary(boost.boston)
@

We see that \texttt{lstat} and \texttt{rm} are by far the most important variables. We can also produce \textit{partial dependence plots} for these two variables. These plots illustrate the marginal effect of the selected variables on the response after \textit{integrating} out the other variables. In this case, as we might expect, median house prices are increasing with \texttt{rm} and decreasing with \texttt{lstat}.

<<>>=
par(mfrow=c(1,2))
plot(boost.boston, i="rm")
plot(boost.boston, i="lstat")
@

We now use the boosted model to predict \texttt{medv} on the test set:

<<>>=
yhat.boost=predict(boost.boston, newdata=Boston[-train,],
                   n.trees=5000)
mean((yhat.boost-boston.test)^2)
@

The test MSE obtained is 11.8; similar to the test MSE for random forests and superior to that for bagging. If we want to, we can perform boosting with a different value of the shrinkage parameter $\lambda$. The default value is 0.0001, but this is easily modified. Here we take $\lambda = 0.2$.

<<>>=
boost.boston = gbm(medv~., data = Boston[train,], 
                   distribution="gaussian",
                   n.trees=5000, 
                   interaction.depth=4, 
                   shrinkage=0.2,
                   verbose=F)
yhat.boost = predict(boost.boston, 
                     newdata=Boston[-train,],
                     n.trees=5000)
mean((yhat.boost-boston.test)^2)
@

In this case, using $\lambda = 0.2$ leads to a slightly lower test MSE than $\lambda = 0.001$.
\end{document}
